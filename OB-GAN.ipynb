{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# OB-GAN Implementation in Pytorch\n",
    "\n",
    "Authors: \n",
    "$\\\\ Om \\; Guin \\\\\n",
    "Pranav \\; Sambhu\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Defining the Model Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# los dependencies sus\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "import torchvision.utils as vutils\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Generator 1 (7 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GeneratorLung(nn.Module):\n",
    "    def __init__(self, z_dim, channels_img, features_g):\n",
    "        super(GeneratorLung, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            self._block(z_dim, features_g * 32, 7, 1, 0),\n",
    "            self._block(features_g * 32, features_g * 16, 3, 2, 1),\n",
    "            self._block(features_g * 16, features_g * 8, 3, 2, 1),\n",
    "            self._block(features_g * 8, features_g * 4, 3, 2, 1),\n",
    "            self._block(features_g * 4, features_g * 2, 2, 2, 1),\n",
    "            nn.ConvTranspose2d(features_g * 2, channels_img, kernel_size=4, stride=2, padding=1, ),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Generator 2 (3 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GeneratorNodule(nn.Module):\n",
    "    def __init__(self, z_dim, channels_img, features_g):\n",
    "        super(GeneratorNodule, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            self._block(z_dim, features_g * 2, 2, 1, 0),\n",
    "            nn.ConvTranspose2d(features_g * 2, channels_img, kernel_size=2, stride=2, padding=1, ),  # img:54x54\n",
    "            nn.Sigmoid(),\n",
    "\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # load Faster RCNN pre-trained model\n",
    "        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "        # get the number of input features\n",
    "        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
    "        # define a new head for the detector with required number of classes\n",
    "        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    def forward(self, images, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (list[Tensor]): images to be processed\n",
    "            targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n",
    "                boxes: float32 - torch.Tensor([[xmin1, ymin1, xmax1, ymax1], [xmin2, ymin2, xmax2, ymax2], ...])\n",
    "                area = area of boxes\n",
    "                labels = labels of boxes\n",
    "                iscrowd\n",
    "                image_id\n",
    "        \"\"\"\n",
    "\n",
    "        loss_dict = self.model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        return loss_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class GAN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(GAN, self).__init__()\n",
    "\n",
    "        self.generatorlung = GeneratorLung(z_dim=27, channels_img=1, features_g=32)\n",
    "        self.generatornodule = GeneratorNodule(z_dim=27, channels_img=1, features_g=32)\n",
    "        self.discriminator = Discriminator(num_classes=num_classes)\n",
    "\n",
    "    def forward_one(self, image, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (Tensor): image to be processed\n",
    "            targets (Dict[str, Tensor]): ground-truth boxes present in the image (optional)\n",
    "        \"\"\"\n",
    "\n",
    "        noise = torch.randn(1, 27, 27, 27)\n",
    "\n",
    "        generated_lungimage = self.generatorlung(noise)[0]\n",
    "        generated_nodule = self.generatornodule(noise)[0]\n",
    "\n",
    "        # Image doesnt need autograd\n",
    "        image.requires_grad = False\n",
    "\n",
    "        generatorlung_loss = F.binary_cross_entropy(generated_lungimage, image)\n",
    "\n",
    "        avg_nodule = torch.full((1, 54, 54), 0.6)\n",
    "\n",
    "        # Might be scuffed\n",
    "        generatornodule_loss = F.binary_cross_entropy(avg_nodule, generated_nodule)\n",
    "\n",
    "        # This is to allow autograd to do autograd of generated lung image without interference and not mess up the computation graph\n",
    "        generated_lungimage_with_nodules = generated_lungimage.clone().detach()\n",
    "\n",
    "        # Replace the pixels in the bounding box with the nodule\n",
    "        for bbox in target[\"boxes\"]:\n",
    "            xmin = int(bbox[0].item())\n",
    "            ymin = int(bbox[1].item())\n",
    "            xmax = int(bbox[2].item())\n",
    "            ymax = int(bbox[3].item())\n",
    "\n",
    "            resize_transform = torchvision.transforms.Resize(size=(ymax-ymin, xmax-xmin))\n",
    "\n",
    "            resized_generated_nodule = resize_transform(generated_nodule)\n",
    "\n",
    "            generated_lungimage_with_nodules[:, ymin:ymax, xmin:xmax] = resized_generated_nodule\n",
    "\n",
    "\n",
    "        loss_value_bbox_realimage_discriminator = self.discriminator(images=[image], targets=[target])\n",
    "        loss_value_bbox_generatedimage_discriminator = self.discriminator(images=[generated_lungimage_with_nodules], targets=[target])\n",
    "\n",
    "        discriminator_loss = loss_value_bbox_realimage_discriminator - (generatorlung_loss + generatornodule_loss) + loss_value_bbox_generatedimage_discriminator\n",
    "\n",
    "        return [generatorlung_loss, generatornodule_loss, discriminator_loss]\n",
    "\n",
    "    def forward(self, images, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (list[Tensor]): images to be processed\n",
    "            targets (list[Dict[str, Tensor]]): ground-truth boxes present in the images (optional)\n",
    "        \"\"\"\n",
    "\n",
    "        all_losses = [self.forward_one(images[i], targets[i]) for i in range(len(images))]\n",
    "        sum_generatorlung_loss = sum([all_losses[i][0] for i in range(len(all_losses))])\n",
    "        sum_generatornodule_loss = sum([all_losses[i][1] for i in range(len(all_losses))])\n",
    "        sum_discriminator_loss = sum([all_losses[i][2] for i in range(len(all_losses))])\n",
    "\n",
    "        return [sum_generatorlung_loss, sum_generatornodule_loss, sum_discriminator_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from config import (\n",
    "    DEVICE, NUM_CLASSES, NUM_EPOCHS, OUT_DIR,\n",
    "    VISUALIZE_TRANSFORMED_IMAGES, NUM_WORKERS,\n",
    ")\n",
    "from custom_utils import Averager, SaveBestModel, save_model, save_loss_plot\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import (\n",
    "    create_train_dataset, create_valid_dataset,\n",
    "    create_train_loader, create_valid_loader\n",
    ")\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# function for running training iterations\n",
    "def train(train_data_loader, model):\n",
    "    print('Training')\n",
    "    global train_itr\n",
    "    global train_loss_list\n",
    "\n",
    "    # initialize tqdm progress bar\n",
    "    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
    "\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        optimizer_genlung.zero_grad()\n",
    "        optimizer_gennodule.zero_grad()\n",
    "        optimizer_disc.zero_grad()\n",
    "\n",
    "        images, targets = data\n",
    "\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        generatorlung_loss, generatornodule_loss, discriminator_loss = model(images, targets)\n",
    "\n",
    "        generatorlung_loss_value = generatorlung_loss.item()\n",
    "        generatorlung_loss_list.append(generatorlung_loss_value)\n",
    "        generatorlung_loss_hist.send(generatorlung_loss_value)\n",
    "\n",
    "        generatornodule_loss_value = generatornodule_loss.item()\n",
    "        generatornodule_loss_list.append(generatornodule_loss_value)\n",
    "        generatornodule_loss_hist.send(generatornodule_loss_value)\n",
    "\n",
    "        discriminator_loss_value = discriminator_loss.item()\n",
    "        discriminator_loss_list.append(discriminator_loss_value)\n",
    "        discriminator_loss_hist.send(discriminator_loss_value)\n",
    "\n",
    "        generatorlung_loss.backward()\n",
    "        generatornodule_loss.backward()\n",
    "        discriminator_loss.backward()\n",
    "\n",
    "        optimizer_genlung.step()\n",
    "        optimizer_gennodule.step()\n",
    "        optimizer_disc.step()\n",
    "\n",
    "        train_itr += 1\n",
    "\n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(\n",
    "            desc=f\"Loss for Lung Generator: {generatorlung_loss_value:.4f}. Loss for Nodule Generator: {generatornodule_loss_value:.4f}. Loss for Discriminator: {discriminator_loss_value:.4f}.\")\n",
    "\n",
    "    return [generatorlung_loss_list, generatornodule_loss_list, discriminator_loss_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# function for running validation iterations\n",
    "def validate(valid_data_loader, model):\n",
    "    print('Validating')\n",
    "    global val_itr\n",
    "    global val_loss_list\n",
    "\n",
    "    # initialize tqdm progress bar\n",
    "    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n",
    "\n",
    "    for i, data in enumerate(prog_bar):\n",
    "        images, targets = data\n",
    "\n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generatorlung_loss, generatornodule_loss, discriminator_loss = model(images, targets)\n",
    "\n",
    "        generatorlung_loss_value = generatorlung_loss.item()\n",
    "        val_generatorlung_loss_list.append(generatorlung_loss_value)\n",
    "        val_generatorlung_loss_hist.send(generatorlung_loss_value)\n",
    "\n",
    "        generatornodule_loss_value = generatornodule_loss.item()\n",
    "        val_generatornodule_loss_list.append(generatornodule_loss_value)\n",
    "        val_generatornodule_loss_hist.send(generatornodule_loss_value)\n",
    "\n",
    "        discriminator_loss_value = discriminator_loss.item()\n",
    "        val_discriminator_loss_list.append(discriminator_loss_value)\n",
    "        val_discriminator_loss_hist.send(discriminator_loss_value)\n",
    "\n",
    "        val_itr += 1\n",
    "\n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(\n",
    "            desc=f\"Loss for Lung Generator: {generatorlung_loss_value:.4f}. Loss for Nodule Generator: {generatornodule_loss_value:.4f}. Loss for Discriminator: {discriminator_loss_value:.4f}.\")\n",
    "\n",
    "    return [val_generatorlung_loss_list, val_generatornodule_loss_list, val_discriminator_loss_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 1723\n",
      "Number of validation samples: 217\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Pycharm Projects\\OB-GAN\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Pycharm Projects\\OB-GAN\\venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 of 15\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/216 [00:17<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7235195f68314d629b5cc4abd53cd0c5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[23], line 69\u001B[0m\n\u001B[0;32m     67\u001B[0m \u001B[38;5;66;03m# start timer and carry out training and validation\u001B[39;00m\n\u001B[0;32m     68\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m---> 69\u001B[0m generatorlung_loss_list, generatornodule_loss_list, discriminator_loss_list \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     70\u001B[0m val_generatorlung_loss_list, val_generatornodule_loss_list, val_discriminator_loss_list \u001B[38;5;241m=\u001B[39m validate(valid_loader, model)\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEpoch #\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepoch\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Train. Loss for Lung Generator: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgeneratorlung_loss_hist\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Loss for Nodule Generator: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgeneratornodule_loss_hist\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. Loss for Discriminator: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdiscriminator_loss_hist\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[14], line 20\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(train_data_loader, model)\u001B[0m\n\u001B[0;32m     17\u001B[0m images \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(image\u001B[38;5;241m.\u001B[39mto(DEVICE) \u001B[38;5;28;01mfor\u001B[39;00m image \u001B[38;5;129;01min\u001B[39;00m images)\n\u001B[0;32m     18\u001B[0m targets \u001B[38;5;241m=\u001B[39m [{k: v\u001B[38;5;241m.\u001B[39mto(DEVICE) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m t\u001B[38;5;241m.\u001B[39mitems()} \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m targets]\n\u001B[1;32m---> 20\u001B[0m generatorlung_loss, generatornodule_loss, discriminator_loss \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m generatorlung_loss_value \u001B[38;5;241m=\u001B[39m generatorlung_loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m     23\u001B[0m generatorlung_loss_list\u001B[38;5;241m.\u001B[39mappend(generatorlung_loss_value)\n",
      "File \u001B[1;32mC:\\Pycharm Projects\\OB-GAN\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[1;32mIn[22], line 48\u001B[0m, in \u001B[0;36mGAN.forward\u001B[1;34m(self, images, targets)\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, images, targets):\n\u001B[0;32m     42\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;124;03m        images (list[Tensor]): images to be processed\u001B[39;00m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;124;03m        targets (list[Dict[str, Tensor]]): ground-truth boxes present in the images (optional)\u001B[39;00m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 48\u001B[0m     all_losses \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward_one(images[i], targets[i]) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(images))]\n\u001B[0;32m     49\u001B[0m     sum_generatorlung_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m([all_losses[i][\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(all_losses))])\n\u001B[0;32m     50\u001B[0m     sum_generatornodule_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m([all_losses[i][\u001B[38;5;241m1\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(all_losses))])\n",
      "Cell \u001B[1;32mIn[22], line 48\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, images, targets):\n\u001B[0;32m     42\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;124;03m        images (list[Tensor]): images to be processed\u001B[39;00m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;124;03m        targets (list[Dict[str, Tensor]]): ground-truth boxes present in the images (optional)\u001B[39;00m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 48\u001B[0m     all_losses \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward_one\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(images))]\n\u001B[0;32m     49\u001B[0m     sum_generatorlung_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m([all_losses[i][\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(all_losses))])\n\u001B[0;32m     50\u001B[0m     sum_generatornodule_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m([all_losses[i][\u001B[38;5;241m1\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(all_losses))])\n",
      "Cell \u001B[1;32mIn[22], line 23\u001B[0m, in \u001B[0;36mGAN.forward_one\u001B[1;34m(self, image, target)\u001B[0m\n\u001B[0;32m     19\u001B[0m generated_nodule \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgeneratornodule(noise)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     21\u001B[0m generatorlung_loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mbinary_cross_entropy(generated_lungimage, image)\n\u001B[1;32m---> 23\u001B[0m real_nodules \u001B[38;5;241m=\u001B[39m [image[bbox[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;241m.\u001B[39mitem():bbox[\u001B[38;5;241m3\u001B[39m]\u001B[38;5;241m.\u001B[39mitem(), bbox[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mitem():bbox[\u001B[38;5;241m2\u001B[39m]\u001B[38;5;241m.\u001B[39mitem()] \u001B[38;5;28;01mfor\u001B[39;00m bbox \u001B[38;5;129;01min\u001B[39;00m target[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mboxes\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\n\u001B[0;32m     24\u001B[0m avg_nodule \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(real_nodules)\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(real_nodules)\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m# Might be scuffed\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[22], line 23\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     19\u001B[0m generated_nodule \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgeneratornodule(noise)[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m     21\u001B[0m generatorlung_loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mbinary_cross_entropy(generated_lungimage, image)\n\u001B[1;32m---> 23\u001B[0m real_nodules \u001B[38;5;241m=\u001B[39m [\u001B[43mimage\u001B[49m\u001B[43m[\u001B[49m\u001B[43mbbox\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\u001B[43mbbox\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbbox\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\u001B[43mbbox\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m bbox \u001B[38;5;129;01min\u001B[39;00m target[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mboxes\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\n\u001B[0;32m     24\u001B[0m avg_nodule \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(real_nodules)\u001B[38;5;241m/\u001B[39m\u001B[38;5;28mlen\u001B[39m(real_nodules)\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m# Might be scuffed\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    train_dataset = create_train_dataset()\n",
    "    valid_dataset = create_valid_dataset()\n",
    "    train_loader = create_train_loader(train_dataset, NUM_WORKERS)\n",
    "    valid_loader = create_valid_loader(valid_dataset, NUM_WORKERS)\n",
    "    print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "    print(f\"Number of validation samples: {len(valid_dataset)}\\n\")\n",
    "\n",
    "    # initialize the model and move to the computation device\n",
    "    model = GAN(num_classes=NUM_CLASSES)\n",
    "    model = model.to(DEVICE)\n",
    "    # get the model parameters\n",
    "    params_genlung = [p for p in model.generatorlung.parameters() if p.requires_grad]\n",
    "    optimizer_genlung = torch.optim.SGD(params_genlung, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    params_gennodule = [p for p in model.generatornodule.parameters() if p.requires_grad]\n",
    "    optimizer_gennodule = torch.optim.SGD(params_gennodule, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    params_disc = [p for p in model.discriminator.parameters() if p.requires_grad]\n",
    "    optimizer_disc = torch.optim.SGD(params_disc, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    # initialize the Averager class\n",
    "    generatorlung_loss_hist = Averager()\n",
    "    generatornodule_loss_hist = Averager()\n",
    "    discriminator_loss_hist = Averager()\n",
    "\n",
    "    val_generatorlung_loss_hist = Averager()\n",
    "    val_generatornodule_loss_hist = Averager()\n",
    "    val_discriminator_loss_hist = Averager()\n",
    "\n",
    "    train_itr = 1\n",
    "    val_itr = 1\n",
    "    # train and validation loss lists to store loss values of all...\n",
    "    # ... iterations till ena and plot graphs for all iterations\n",
    "    generatorlung_loss_list = []\n",
    "    generatornodule_loss_list = []\n",
    "    discriminator_loss_list = []\n",
    "\n",
    "    val_generatorlung_loss_list = []\n",
    "    val_generatornodule_loss_list = []\n",
    "    val_discriminator_loss_list = []\n",
    "\n",
    "    # name to save the trained model with\n",
    "    MODEL_NAME = 'scl-DC7G3GANN'\n",
    "\n",
    "    # whether to show transformed images from data loader or not\n",
    "    if VISUALIZE_TRANSFORMED_IMAGES:\n",
    "        from custom_utils import show_tranformed_image\n",
    "\n",
    "        show_tranformed_image(train_loader)\n",
    "\n",
    "    # initialize SaveBestModel class\n",
    "    save_best_model = SaveBestModel()\n",
    "\n",
    "    # start the training epochs\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEPOCH {epoch + 1} of {NUM_EPOCHS}\")\n",
    "\n",
    "        # reset the training and validation loss histories for the current epoch\n",
    "        generatorlung_loss_hist.reset()\n",
    "        generatornodule_loss_hist.reset()\n",
    "        discriminator_loss_hist.reset()\n",
    "\n",
    "        val_generatorlung_loss_hist.reset()\n",
    "        val_generatornodule_loss_hist.reset()\n",
    "        val_discriminator_loss_hist.reset()\n",
    "\n",
    "        # start timer and carry out training and validation\n",
    "        start = time.time()\n",
    "        generatorlung_loss_list, generatornodule_loss_list, discriminator_loss_list = train(train_loader, model)\n",
    "        val_generatorlung_loss_list, val_generatornodule_loss_list, val_discriminator_loss_list = validate(valid_loader,\n",
    "                                                                                                           model)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch #{epoch + 1}. Train. Loss for Lung Generator: {generatorlung_loss_hist:.4f}. Loss for Nodule Generator: {generatornodule_loss_hist:.4f}. Loss for Discriminator: {discriminator_loss_hist:.4f}.\")\n",
    "        print(\n",
    "            f\"Epoch #{epoch + 1}. Validation. Loss for Lung Generator: {val_generatorlung_loss_hist:.4f}. Loss for Nodule Generator: {val_generatornodule_loss_hist:.4f}. Loss for Discriminator: {val_discriminator_loss_hist:.4f}.\")\n",
    "\n",
    "        end = time.time()\n",
    "        print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch}\")\n",
    "\n",
    "        # save loss plot\n",
    "        save_loss_plot(OUT_DIR, generatorlung_loss_list, val_generatorlung_loss_list)\n",
    "        save_loss_plot(OUT_DIR, generatornodule_loss_list, val_generatornodule_loss_list)\n",
    "        save_loss_plot(OUT_DIR, discriminator_loss_list, val_discriminator_loss_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}