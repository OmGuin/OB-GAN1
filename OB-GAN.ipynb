{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OB-GAN Implementation in Pytorch\n",
    "\n",
    "Authors: \n",
    "$\\\\ Om \\; Guin \\\\\n",
    "Pranav \\; Sambhu\n",
    "$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Model Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# los dependencies sus \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as dset \n",
    "import torch.nn as nn\n",
    "import torchvision.utils as vutils\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split  \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator 1 (7 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorLung(nn.Module):\n",
    "    def __init__(self, z_dim, channels_img, features_g):\n",
    "        super(GeneratorLung, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            self._block(z_dim, features_g*32, 7, 1, 0),\n",
    "            self._block(features_g*32, features_g*16, 3, 2, 1),\n",
    "            self._block(features_g*16, features_g*8, 3, 2, 1),\n",
    "            self._block(features_g*8, features_g*4, 3, 2, 1),\n",
    "            self._block(features_g*4, features_g*2, 2, 2, 1),\n",
    "            nn.ConvTranspose2d(features_g*2, channels_img, kernel_size=4, stride=2, padding=1,),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generator 2 (3 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorNodule(nn.Module):\n",
    "    def __init__(self, z_dim, channels_img, features_g):\n",
    "        super(GeneratorNodule, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            self._block(z_dim, features_g*2, 2, 1, 0),\n",
    "            nn.ConvTranspose2d(features_g*2, channels_img, kernel_size=2, stride=2, padding=1,), #img:54x54\n",
    "            nn.Tanh(),\n",
    "        \n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # load Faster RCNN pre-trained model\n",
    "        self.model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "        # get the number of input features \n",
    "        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
    "        # define a new head for the detector with required number of classes\n",
    "        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    def forward(self, images, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (list[Tensor]): images to be processed\n",
    "            targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n",
    "                boxes: float32 - torch.Tensor([[xmin1, ymin1, xmax1, ymax1], [xmin2, ymin2, xmax2, ymax2], ...])\n",
    "                area = area of boxes\n",
    "                labels = labels of boxes\n",
    "                iscrowd\n",
    "                image_id\n",
    "        \"\"\"\n",
    "\n",
    "        loss_dict = self.model(images, targets)\n",
    "\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        return loss_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(GAN, self).__init__()\n",
    "\n",
    "        self.generatorlung = GeneratorLung(z_dim=27, channels_img=1, features_g=32)\n",
    "        self.generatornodule = GeneratorNodule(z_dim=27, channels_img=1, features_g=32)\n",
    "        self.discriminator = Discriminator(num_classes=num_classes)\n",
    "    \n",
    "    def forward_one(self, image, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (Tensor): image to be processed\n",
    "            targets (Dict[str, Tensor]): ground-truth boxes present in the image (optional)\n",
    "        \"\"\"\n",
    "\n",
    "        noise = torch.randn(1, 27, 27, 27)\n",
    "\n",
    "        generated_lungimage = self.generatorlung(noise)\n",
    "        generated_nodule = self.generatornodule(noise)\n",
    "\n",
    "        generatorlung_loss = F.binary_cross_entropy(generated_lungimage, image)\n",
    "\n",
    "        real_nodules = [image[bbox[1]:bbox[3], bbox[0]:bbox[2]] for bbox in target]\n",
    "        avg_nodule = sum(real_nodules)/len(real_nodules)\n",
    "\n",
    "        # Might be scuffed\n",
    "        generatornodule_loss = F.binary_cross_entropy(avg_nodule, real_nodules)\n",
    "\n",
    "        # Replace the pixels in the bounding box with the nodule\n",
    "        for bbox in target:\n",
    "            generated_lungimage[bbox[1]:bbox[3], bbox[0]:bbox[2]] = generated_nodule\n",
    "\n",
    "        loss_value_bbox_realimage_discriminator = self.discriminator(images=[image], targets=[target])\n",
    "        loss_value_bbox_generatedimage_discriminator = self.discriminator(images=[generated_lungimage], targets=[target])\n",
    "\n",
    "        discriminator_loss = loss_value_bbox_realimage_discriminator - (generatorlung_loss + generatornodule_loss) + loss_value_bbox_generatedimage_discriminator\n",
    "\n",
    "        return [generatorlung_loss, generatornodule_loss, discriminator_loss]\n",
    "\n",
    "    \n",
    "    def forward(self, images, targets):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images (list[Tensor]): images to be processed\n",
    "            targets (list[Dict[str, Tensor]]): ground-truth boxes present in the images (optional)\n",
    "        \"\"\"\n",
    "\n",
    "        all_losses = [self.forward_one(images[i], targets[i]) for i in range(len(images))]\n",
    "        sum_generatorlung_loss = sum([all_losses[i][0] for i in range(len(all_losses))])\n",
    "        sum_generatornodule_loss = sum([all_losses[i][1] for i in range(len(all_losses))])\n",
    "        sum_discriminator_loss = sum([all_losses[i][2] for i in range(len(all_losses))])\n",
    "\n",
    "        return [sum_generatorlung_loss, sum_generatornodule_loss, sum_discriminator_loss]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import (\n",
    "    DEVICE, NUM_CLASSES, NUM_EPOCHS, OUT_DIR,\n",
    "    VISUALIZE_TRANSFORMED_IMAGES, NUM_WORKERS,\n",
    ")\n",
    "from custom_utils import Averager, SaveBestModel, save_model, save_loss_plot\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import (\n",
    "    create_train_dataset, create_valid_dataset, \n",
    "    create_train_loader, create_valid_loader\n",
    ")\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for running training iterations\n",
    "def train(train_data_loader, model):\n",
    "    print('Training')\n",
    "    global train_itr\n",
    "    global train_loss_list\n",
    "    \n",
    "     # initialize tqdm progress bar\n",
    "    prog_bar = tqdm(train_data_loader, total=len(train_data_loader))\n",
    "    \n",
    "    for i, data in enumerate(prog_bar):\n",
    "        optimizer_genlung.zero_grad()\n",
    "        optimizer_gennodule.zero_grad()\n",
    "        optimizer_disc.zero_grad()\n",
    "\n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        generatorlung_loss, generatornodule_loss, discriminator_loss = model(images, targets)\n",
    "\n",
    "        generatorlung_loss_value = generatorlung_loss.item()\n",
    "        generatorlung_loss_list.append(generatorlung_loss_value)\n",
    "        generatorlung_loss_hist.send(generatorlung_loss_value)\n",
    "\n",
    "        generatornodule_loss_value = generatornodule_loss.item()\n",
    "        generatornodule_loss_list.append(generatornodule_loss_value)\n",
    "        generatornodule_loss_hist.send(generatornodule_loss_value)\n",
    "\n",
    "        discriminator_loss_value = discriminator_loss.item()\n",
    "        discriminator_loss_list.append(discriminator_loss_value)\n",
    "        discriminator_loss_hist.send(discriminator_loss_value)\n",
    "        \n",
    "        generatorlung_loss.backward()\n",
    "        generatornodule_loss.backward()\n",
    "        discriminator_loss.backward()\n",
    "\n",
    "        optimizer_genlung.step()\n",
    "        optimizer_gennodule.step()\n",
    "        optimizer_disc.step()\n",
    "\n",
    "        train_itr += 1\n",
    "    \n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(desc=f\"Loss for Lung Generator: {generatorlung_loss_value:.4f}. Loss for Nodule Generator: {generatornodule_loss_value:.4f}. Loss for Discriminator: {discriminator_loss_value:.4f}.\")\n",
    "        \n",
    "    return [generatorlung_loss_list, generatornodule_loss_list, discriminator_loss_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for running validation iterations\n",
    "def validate(valid_data_loader, model):\n",
    "    print('Validating')\n",
    "    global val_itr\n",
    "    global val_loss_list\n",
    "    \n",
    "    # initialize tqdm progress bar\n",
    "    prog_bar = tqdm(valid_data_loader, total=len(valid_data_loader))\n",
    "    \n",
    "    for i, data in enumerate(prog_bar):\n",
    "        images, targets = data\n",
    "        \n",
    "        images = list(image.to(DEVICE) for image in images)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generatorlung_loss, generatornodule_loss, discriminator_loss = model(images, targets)\n",
    "\n",
    "        generatorlung_loss_value = generatorlung_loss.item()\n",
    "        val_generatorlung_loss_list.append(generatorlung_loss_value)\n",
    "        val_generatorlung_loss_hist.send(generatorlung_loss_value)\n",
    "\n",
    "        generatornodule_loss_value = generatornodule_loss.item()\n",
    "        val_generatornodule_loss_list.append(generatornodule_loss_value)\n",
    "        val_generatornodule_loss_hist.send(generatornodule_loss_value)\n",
    "\n",
    "        discriminator_loss_value = discriminator_loss.item()\n",
    "        val_discriminator_loss_list.append(discriminator_loss_value)\n",
    "        val_discriminator_loss_hist.send(discriminator_loss_value)\n",
    "\n",
    "        val_itr += 1\n",
    "    \n",
    "        # update the loss value beside the progress bar for each iteration\n",
    "        prog_bar.set_description(desc=f\"Loss for Lung Generator: {generatorlung_loss_value:.4f}. Loss for Nodule Generator: {generatornodule_loss_value:.4f}. Loss for Discriminator: {discriminator_loss_value:.4f}.\")\n",
    "        \n",
    "    return [val_generatorlung_loss_list, val_generatornodule_loss_list, val_discriminator_loss_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 1723\n",
      "Number of validation samples: 217\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\om\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\om\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1 of 15\n",
      "Training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60408313b23b4010bb7ad6a12e5d03b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/216 [00:32<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([3, 1024, 1024])) that is different to the input size (torch.Size([1, 1, 1024, 1024])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Downloads\\ScienceFair24\\scl-DC7G3GANN.ipynb Cell 12\u001b[0m in \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39m# start timer and carry out training and validation\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m generatorlung_loss_list, generatornodule_loss_list, discriminator_loss_list \u001b[39m=\u001b[39m train(train_loader, model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m val_generatorlung_loss_list, val_generatornodule_loss_list, val_discriminator_loss_list \u001b[39m=\u001b[39m validate(valid_loader, model)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch #\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m. Train. Loss for Lung Generator: \u001b[39m\u001b[39m{\u001b[39;00mgeneratorlung_loss_hist\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m. Loss for Nodule Generator: \u001b[39m\u001b[39m{\u001b[39;00mgeneratornodule_loss_hist\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m. Loss for Discriminator: \u001b[39m\u001b[39m{\u001b[39;00mdiscriminator_loss_hist\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32md:\\Downloads\\ScienceFair24\\scl-DC7G3GANN.ipynb Cell 12\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m images \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(image\u001b[39m.\u001b[39mto(DEVICE) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m targets \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39mto(DEVICE) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m targets]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m generatorlung_loss, generatornodule_loss, discriminator_loss \u001b[39m=\u001b[39m model(images, targets)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m generatorlung_loss_value \u001b[39m=\u001b[39m generatorlung_loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m generatorlung_loss_list\u001b[39m.\u001b[39mappend(generatorlung_loss_value)\n",
      "File \u001b[1;32mc:\\Users\\om\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\Downloads\\ScienceFair24\\scl-DC7G3GANN.ipynb Cell 12\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, images, targets):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m        images (list[Tensor]): images to be processed\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m        targets (list[Dict[str, Tensor]]): ground-truth boxes present in the images (optional)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     all_losses \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_one(images[i], targets[i]) \u001b[39mfor\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(images))]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     sum_generatorlung_loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m([all_losses[i][\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(all_losses))])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     sum_generatornodule_loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m([all_losses[i][\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(all_losses))])\n",
      "\u001b[1;32md:\\Downloads\\ScienceFair24\\scl-DC7G3GANN.ipynb Cell 12\u001b[0m in \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, images, targets):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m        images (list[Tensor]): images to be processed\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m        targets (list[Dict[str, Tensor]]): ground-truth boxes present in the images (optional)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     all_losses \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_one(images[i], targets[i]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(images))]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     sum_generatorlung_loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m([all_losses[i][\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(all_losses))])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     sum_generatornodule_loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m([all_losses[i][\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(all_losses))])\n",
      "\u001b[1;32md:\\Downloads\\ScienceFair24\\scl-DC7G3GANN.ipynb Cell 12\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m generated_lungimage \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneratorlung(noise)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m generated_nodule \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneratornodule(noise)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m generatorlung_loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mbinary_cross_entropy(generated_lungimage, image)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m real_nodules \u001b[39m=\u001b[39m [image[bbox[\u001b[39m1\u001b[39m]:bbox[\u001b[39m3\u001b[39m], bbox[\u001b[39m0\u001b[39m]:bbox[\u001b[39m2\u001b[39m]] \u001b[39mfor\u001b[39;00m bbox \u001b[39min\u001b[39;00m target]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Downloads/ScienceFair24/scl-DC7G3GANN.ipynb#X34sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m avg_nodule \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(real_nodules)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(real_nodules)\n",
      "File \u001b[1;32mc:\\Users\\om\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:3089\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3087\u001b[0m     reduction_enum \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3088\u001b[0m \u001b[39mif\u001b[39;00m target\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize():\n\u001b[1;32m-> 3089\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   3090\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUsing a target size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) that is different to the input size (\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   3091\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease ensure they have the same size.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(target\u001b[39m.\u001b[39msize(), \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m   3092\u001b[0m     )\n\u001b[0;32m   3094\u001b[0m \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3095\u001b[0m     new_size \u001b[39m=\u001b[39m _infer_size(target\u001b[39m.\u001b[39msize(), weight\u001b[39m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([3, 1024, 1024])) that is different to the input size (torch.Size([1, 1, 1024, 1024])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_dataset = create_train_dataset()\n",
    "    valid_dataset = create_valid_dataset()\n",
    "    train_loader = create_train_loader(train_dataset, NUM_WORKERS)\n",
    "    valid_loader = create_valid_loader(valid_dataset, NUM_WORKERS)\n",
    "    print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "    print(f\"Number of validation samples: {len(valid_dataset)}\\n\")\n",
    "\n",
    "    # initialize the model and move to the computation device\n",
    "    model = GAN(num_classes=NUM_CLASSES)\n",
    "    model = model.to(DEVICE)\n",
    "    # get the model parameters\n",
    "    params_genlung = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "    optimizer_genlung = torch.optim.SGD(params_genlung, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    params_gennodule = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "    optimizer_gennodule = torch.optim.SGD(params_gennodule, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    params_disc = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "    optimizer_disc = torch.optim.SGD(params_disc, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    # initialize the Averager class\n",
    "    generatorlung_loss_hist = Averager()\n",
    "    generatornodule_loss_hist = Averager()\n",
    "    discriminator_loss_hist = Averager()\n",
    "\n",
    "    val_generatorlung_loss_hist = Averager()\n",
    "    val_generatornodule_loss_hist = Averager()\n",
    "    val_discriminator_loss_hist = Averager()\n",
    "    \n",
    "    train_itr = 1\n",
    "    val_itr = 1\n",
    "    # train and validation loss lists to store loss values of all...\n",
    "    # ... iterations till ena and plot graphs for all iterations\n",
    "    generatorlung_loss_list = []\n",
    "    generatornodule_loss_list= []\n",
    "    discriminator_loss_list =[]\n",
    "\n",
    "    val_generatorlung_loss_list = []\n",
    "    val_generatornodule_loss_list = []\n",
    "    val_discriminator_loss_list = []\n",
    "\n",
    "    # name to save the trained model with\n",
    "    MODEL_NAME = 'scl-DC7G3GANN'\n",
    "\n",
    "    # whether to show transformed images from data loader or not\n",
    "    if VISUALIZE_TRANSFORMED_IMAGES:\n",
    "        from custom_utils import show_tranformed_image\n",
    "        show_tranformed_image(train_loader)\n",
    "\n",
    "    # initialize SaveBestModel class\n",
    "    save_best_model = SaveBestModel()\n",
    "\n",
    "    # start the training epochs\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEPOCH {epoch+1} of {NUM_EPOCHS}\")\n",
    "\n",
    "        # reset the training and validation loss histories for the current epoch\n",
    "        generatorlung_loss_hist.reset()\n",
    "        generatornodule_loss_hist.reset()\n",
    "        discriminator_loss_hist.reset()\n",
    "\n",
    "        val_generatorlung_loss_hist.reset()\n",
    "        val_generatornodule_loss_hist.reset()\n",
    "        val_discriminator_loss_hist.reset()\n",
    "\n",
    "        # start timer and carry out training and validation\n",
    "        start = time.time()\n",
    "        generatorlung_loss_list, generatornodule_loss_list, discriminator_loss_list = train(train_loader, model)\n",
    "        val_generatorlung_loss_list, val_generatornodule_loss_list, val_discriminator_loss_list = validate(valid_loader, model)\n",
    "\n",
    "\n",
    "        print(f\"Epoch #{epoch+1}. Train. Loss for Lung Generator: {generatorlung_loss_hist:.4f}. Loss for Nodule Generator: {generatornodule_loss_hist:.4f}. Loss for Discriminator: {discriminator_loss_hist:.4f}.\")\n",
    "        print(f\"Epoch #{epoch+1}. Validation. Loss for Lung Generator: {val_generatorlung_loss_hist:.4f}. Loss for Nodule Generator: {val_generatornodule_loss_hist:.4f}. Loss for Discriminator: {val_discriminator_loss_hist:.4f}.\")\n",
    "\n",
    "        end = time.time()\n",
    "        print(f\"Took {((end - start) / 60):.3f} minutes for epoch {epoch}\")\n",
    "\n",
    "        # save loss plot\n",
    "        save_loss_plot(OUT_DIR, generatorlung_loss_list, val_generatorlung_loss_list)   \n",
    "        save_loss_plot(OUT_DIR, generatornodule_loss_list, val_generatornodule_loss_list)\n",
    "        save_loss_plot(OUT_DIR, discriminator_loss_list, val_discriminator_loss_list)   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
